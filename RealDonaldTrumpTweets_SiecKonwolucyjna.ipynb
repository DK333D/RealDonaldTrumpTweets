{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm-xFcmg2p-N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchtext.vocab import GloVe\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming data and labels_one_hot are already defined\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, labels_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "glove = GloVe(name='6B', dim=100)\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
        "for word, i in word_index.items():\n",
        "    if word in glove.stoi:\n",
        "        embedding_vector = glove[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = True\n",
        "        self.convs = nn.ModuleList(\n",
        "            [nn.Conv1d(in_channels=embed_size, out_channels=c, kernel_size=k) for k, c in zip(kernel_sizes, num_channels)]\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(sum(num_channels), 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).permute(0, 2, 1)\n",
        "        convs = [torch.relu(conv(x)) for conv in self.convs]\n",
        "        pools = [torch.max(conv, dim=2)[0] for conv in convs]\n",
        "        out = torch.cat(pools, dim=1)\n",
        "        out = self.dropout(out)\n",
        "        return self.fc(out)\n",
        "\n",
        "embed_size = 100\n",
        "kernel_sizes = [4, 5, 6]\n",
        "num_channels = [128, 128, 128]\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TextCNN(vocab_size, embed_size, kernel_sizes, num_channels)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in train_loader:\n",
        "            texts, labels = batch\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                texts, labels = batch\n",
        "                texts, labels = texts.to(device), labels.to(device)\n",
        "                outputs = model(texts)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}')\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "def evaluate_accuracy(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in data_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "y_train_integers = np.argmax(y_train, axis=1)\n",
        "y_val_integers = np.argmax(y_val, axis=1)\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.long), torch.tensor(y_train_integers, dtype=torch.long))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.long), torch.tensor(y_val_integers, dtype=torch.long))\n",
        "train_loader = DataLoader(train_dataset, batch_size=494, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=494, shuffle=False)\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.01):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "early_stopping = EarlyStopping(patience=5, min_delta=0.01)\n",
        "\n",
        "num_epochs = 25\n",
        "train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "train_accuracy = evaluate_accuracy(model, train_loader)\n",
        "val_accuracy = evaluate_accuracy(model, val_loader)\n",
        "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
        "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "\n",
        "def predict_sentiment(model, vocab, sentence):\n",
        "    model.eval()\n",
        "    tokens = sentence.lower().split()\n",
        "    indices = [vocab.get(token, 0) for token in tokens]\n",
        "    max_kernel_size = max(kernel_sizes)\n",
        "    if len(indices) < max_kernel_size:\n",
        "        indices.extend([0] * (max_kernel_size - len(indices)))\n",
        "    tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(tensor)\n",
        "        prediction = torch.argmax(output, axis=1).item()\n",
        "    return prediction\n",
        "\n",
        "\n"
      ]
    }
  ]
}